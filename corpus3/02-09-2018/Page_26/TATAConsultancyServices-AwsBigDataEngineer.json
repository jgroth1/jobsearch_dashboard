{"job title": "Aws Big Data Engineer", "company": "TATA Consultancy Services", "city state": "\u2013 Malvern, PA", "rating": "3.6", "job description": "<strong>Experience and Qualifications:</strong>\n\n\n<ul>\n<li>2+ years of hands-on experience designing and deploying an AWS-based application (native/re-factored)</li>\n<li>5+ years with Python, Scala, Spark, Oozie, Big Data</li>\n<li>Expertise in the core AWS services, uses, automation, and architecture best practices</li>\n<li>Proficiency in designing, developing, and deploying cloud-based Big Data solutions using AWS</li>\n<li>Experience with developing and maintaining applications written for Amazon Simple Storage Service, Amazon Simple Queue Service, Amazon Simple Notification Service, Amazon Simple Workflow, API Gateway Service, AWS Elastic Beanstalk, and AWS CloudFormation</li>\n<li>Proficiency in Amazon Compute and Storage Instances</li>\n<li>Experience with S3 Server Side Encryption, IAM, and Policy, CloudTrail, CloudWatch.</li>\n<li>Experience on EMR and (Lambda) Serverless Architecture</li>\n<li>Experience setting up Kinsesis streams and integrating them with CDC (Attunity preferred)</li>\n<li>5+ years working with Big Data (Hadoop, Cloudera, HBase)</li>\n<li>Proficiency on High Available, Fault Tolerant, and DR Architecture</li>\n<li>Good working knowledge and experience working with databases like DynamoDB, S3</li>\n<li>Experience working with Google Doubleclick is highly preferred</li>\n<li>Experience on DevOps CI and CD using Jenkins or Bamboo or Code Deploy</li>\n<li>AWS Developer, Solution Architect Certified a plus but not required</li>\n<li>Experience with Atlassian stack highly preferred</li>\n</ul>\n\n<strong>Job responsibilities:</strong>\n\n\n<ul>\n<li>Design, develop and deliver scalable and automated Data Pipelines to ingest Google Doubleclick data</li>\n<li>Familiarity with ingesting and loading data using Oozie workflow manager and cloud-native ingestion services</li>\n<li>Code and enable Data store on S3</li>\n<li>Leverage IAM roles&amp;policies for service authentication</li>\n<li>Build load, transformation, and validation logic in EMR (Spark/Scala)</li>\n<li>Build necessary infra to provision query cluster using existing architecture</li>\n<li>Migrate OnPrem Hadoop data and queries to AWS</li>\n<li>Promote serverless code where appropriate</li>\n<li>Data Quality evaluations based on the source data</li>\n</ul>"}