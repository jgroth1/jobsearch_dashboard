{"job title": "Principal Data Engineer WotC", "company": "Hasbro, Inc.", "city state": "\u2013 Renton, WA", "rating": "3.7", "job description": "<strong>WHAT'S THIS ROLE ALL ABOUT?</strong>\n<br><br>\nAs a Principal Data Engineer for our scalable data platform you will be the technical expert leading the charge to architect, design and build the core infrastructure of our product so that it can handle the volume, variety, and velocity of the SQL traffic flowing through it, and deliver performance optimization, virtualization, and other value adds to the big-data ecosystem it is deployed in. You must thrive in a fast paced, high-energy, startup-like environment.\n<br><br>\nMajor Accountabilities\n\n\n<ul>\n<li>Integral member of our data ingestion and processing platform team responsible for architecture, design and development.</li>\n<li>Design of highly-scalable, reliable, and performant pipelines to consume, integrate and analyze large volumes of complex data using a variety of best-in-class proprietary and open-source platforms and tools.</li>\n<li>Demonstration of client and solution leadership through strong communication skills to recommend actionable, data-driven insights.</li>\n<li>Collaboration with team members, business stakeholders and data SMEs to elicit, translate, and prescribe requirements.</li>\n<li>Management of technical, project, and organizational complexity, with a proven ability to simplify tough problems, track many moving parts across functional teams and business domains, and influence strategic direction.</li>\n<li>Stay ahead of the technology curve and make recommendations about technologies to build current and future products.</li>\n</ul>\n\n<strong>Qualifications</strong>\n\n\n<ul>\n<li>7+ years of professional IT data work experience</li>\n<li>Programming / Scripting language expertise (Python, Java, Scala, Bash)</li>\n<li>Linux / Windows CLI experience, using PowerShell, bash, or equivalent</li>\n<li>2+ years working with Big Data based systems and tools (Hive, Kafka, Spark, AWS EMR)</li>\n<li>2+ years experience working in the cloud (AWS, Azure)</li>\n<li>Data Concepts (ETL, near-/real-time streaming, data structures, metadata and workflow management)</li>\n<li>Data Integration Tools such as Alooma, Informatica, SSIS, and other standalone or integrated pipeline toolsets.</li>\n<li>Databases/EDW experience including MS SQL, Snowflake, DynamoDB, and NoSQL systems such as MongoDB or Couchbase</li>\n<li>Markup Languages (JSON, XML, YAML)</li>\n<li>Code Management and deployment tools including Git, Jenkins, Docker, Kubernetes (Git/GitHub, TFS)</li>\n<li>Experience working in a DevOps environment responding to a system with 24x7 uptime</li>\n<li>Testing / Data Quality (TDD, unit, regression, automation)</li>\n<li>Solving complex data and technology problems based on business needs</li>\n</ul>\n\nWe are an Equal Opportunity / Affirmative Action Employer.\n<br><br>\nThe above is intended to describe the general content of and the requirements for satisfactory performance in this position. It is not to be construed as an exhaustive statement of the duties, responsibilities, or requirements of the position.<br><br>SDL2017"}