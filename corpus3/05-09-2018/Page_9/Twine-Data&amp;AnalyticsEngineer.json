{"job title": "Data &amp; Analytics Engineer", "company": "Twine", "city state": "\u2013 Los Angeles, CA", "rating": "NA", "job description": "Twine is a leading mobile data platform that works with app publishers who generate mobile data and the apps, agencies, brands, and ad tech companies who need that data for mobile ad targeting, optimization, and measurement. We provide app publishers with a platform to safely generate user insights and an incremental revenue stream while delivering mobile marketers high quality mobile data that boosts campaign ROI, at scale.\n<br><br>\nWe are experiencing great momentum and on the back of our Series A we are aggressively expanding our team. <strong>Our Data &amp; Analytics Engineer</strong> will be the next addition to Twine and will report directly to our CTO, and will be based in Downtown LA.\n<br><br>\nPosition\n<br><br>\nAs a member of the engineering team you will be responsible for designing and developing scalable high-performance applications for processing, transforming, and analyzing multi-TB volumes of data generated by mobile applications, including: location data (GIS), identity, and audience data.\n<br><br>\nYou will also be involved in prototyping and testing new products and services for prospective clients like Google, and building next generation analytics, visualization and BI tools for internal and external stakeholders.\n<br><br>\nAs an early member of our growing engineering team, you will receive enormous exposure to new technologies and methods, and have the opportunity to grow immensely within the Twine organization.\n<br><br>\nWhat you\u2019ll be doing:\n<ul>\n <li>Create and maintain optimal data pipeline architecture</li> <li>Assemble large, complex data sets that meet functional / non-functional business requirements.</li> <li>Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.</li> <li>Executing custom match analytics and retrieving custom data samples from multiple datastores, and working with external engineers at our clients (including Google, Oracle, Verizon, etc.) to ensure successful pilots.</li> <li>Building the next generation of Data Visualization and Analytics tools at Twine, including data density analysis for location Data (clustering, univariate analyses, etc), Time series analytics for API/S2S/SDK health monitoring, etc.</li> <li>Managing and automating data ingestion and data delivery processes for new clients, products and services.</li> <li>Administering cloud server architectures (databases and application servers)</li> <li>Deploying and configuring our monitoring, security, deployment, reporting, and automation tools as required</li> <li>Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS \u2018big data\u2019 technologies.</li> <li>Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.</li> <li>Keep our data separated and secure across national boundaries through multiple AWS regions.</li> <li>Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.</li>\n</ul>\nWhat we\u2019re looking for in an ideal candidate:\n<ul>\n <li>Passionate about coding and solving problems using software and automation.</li> <li>You\u2019re eager to work with the rest of the Twine team and our Clients to solve problems, execute on pilots &amp; integrations, and grow the Twine business.</li> <li>You plow through multiple obstacles in a day, using a combination of persistence, research, problem-solving skills, and your own experience</li> <li>You take pride in your work, and especially work that is completed, tested, and delivered</li> <li>You are a sink for problems, rather than a source. You make your co-workers' jobs easier, not harder</li> <li>You are available for and responsive to questions. You are professional and collegial in your communications</li> <li>You like being the person that others rely on</li> <li>You quickly learn new technologies as needed and recognize that you are engaged in timely, business-critical tasks</li> <li>You are transparent in what you do. You discuss, document, and commit your work as needed</li> <li>You recognize technology is a means to an end, not an end in itself. Tech is always for some end user, not for the engineer</li> <li>Excited to work in DTLA.</li>\n</ul>\n<strong>Requirements</strong>\n<br><br>\nQualifications &amp; experience\n<ul>\n <li>We are looking for a candidate with 4+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:\n\n<ul>\n <li>Experience with big data tools:Spark, Kafka, Hadoop, etc.</li> <li>Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.</li> <li>Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.</li> <li>Experience with AWS cloud services: EC2, EMR, RDS, Redshift,Kinesis, SQS,S3,</li>  <li>Experience with stream-processing systems: Spark-Streaming etc.</li> <li>Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.</li>  <li>Ensure AWS cost is managed effectively by refining the pipelines &amp; execution strategies</li> </ul> </li>\n<li>Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.</li> <li>Experience building and optimizing \u2018big data\u2019 data pipelines, architectures and data sets.</li> <li>Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.</li> <li>Strong analytic skills related to working with unstructured datasets.</li> <li>Build processes supporting data transformation, data structures, metadata, dependency and workload management.</li> <li>A successful history of manipulating, processing and extracting value from large disconnected datasets.</li> <li>Working knowledge of message queuing, stream processing, and highly scalable \u2018big data\u2019 data stores.</li>  <li>Code unit testing.</li> <li>Experience with the following data types, structures and techniques is a big plus:\n\n<ul>\n <li>Time series data</li> <li>Location Data (GIS)</li> <li>ML/AI techniques such as Neural Nets, SVM, Linear/Logistic Regression</li> <li>Association Rule Mining, etc. for both out of sample prediction as well as inference</li></ul> </li>\n<li>Ability and willingness to understand, learn, and use new programming languages quickly.</li> <li>Must be familiar with Linux shell, scripting, process monitoring and management, SSH.</li>\n</ul>\n<strong>Benefits</strong>\n<br><br>\nSalary is based on experience &amp; skill level.\n<ul>\n <li>Health Care Plan (Medical, Dental &amp; Vision)</li> <li>Retirement Plan (401k, IRA)</li> <li>Paid Time Off (Vacation, Sick &amp; Public Holidays)</li> <li>Family Leave (Maternity, Paternity)</li> <li>Work From Home</li> <li>Free Food &amp; Snacks</li> <li>Stock Option Plan</li> <li>Transportation Stipend</li>\n</ul>"}