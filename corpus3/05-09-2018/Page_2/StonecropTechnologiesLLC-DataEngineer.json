{"job title": "Data Engineer", "company": "Stonecrop Technologies LLC", "city state": "\u2013 Petaluma, CA", "rating": "2.5", "job description": "<p>We focus on results, encourage mastery, and enjoy the benefits of being part of something big without the frustrations of corporate work culture.<br>\n<br>\nIntriguing, right?! That's how one of our very own Developers described Stonecrop Technologies.<br>\n<br>\nWe are a recognized leader in transforming cellular and microwave deployments through an innovative technology platform that aligns RF designs, supply chain, and installation processes.<br>\n<br>\nSince 2001, we have worked with large corporations to deliver solutions by speeding the build and upgrade of carrier networks while maintaining a culture that encourages peer collaboration and diverse expertise to drive our mission for relentless innovation to improve performance.<br>\n<br>\nWe believe in being agile as a way to focus on delivering real business value within a culture that gives an opportunity to be innovative and move fast.<br>\n<br>\nThis is an excellent opportunity for an experienced Data Engineer who wants to help build our infrastructure and support our analysts, business users, and clients with access to timely, accurate data. In this role, you will develop, construct, test and maintain the data architecture that supports our BI platform, and manage data flow through our web and mobile applications.<br>\n<br>\nTo excel in this role, you should have experience working on and building data pipelines and ETL processes that are reliable and scalable. You should have a solid understanding of how to access and extract data, have worked on cloud-hosted platforms such as AWS and have the soft skills to work with technical teams at different levels.<br>\n<br>\n<strong>You Are:</strong></p>\n\n<ul>\n\t<li>Enthusiastic about data, its uses and best practices.</li>\n\t<li>A habitual learner who is always looking to grow and add new skills.</li>\n\t<li>Able to translate business requests into reliable, scalable infrastructure and database design.</li>\n</ul>\n\n<p><br>\n<strong>You Have:</strong></p>\n\n<ul>\n\t<li>Experience in Airflow, Git, the Linux command line and working with ETL tools.</li>\n\t<li>Strong knowledge of AWS platform and technologies, particularly AWS streaming data solutions and Redshift.</li>\n\t<li>Several years experience writing Python for data processing and API integration. Ruby / Rails knowledge is a bonus.</li>\n\t<li>Experience optimizing SQL queries in Postgresql / Redshift.</li>\n\t<li>BS or MS in Computer Science.</li>\n</ul>\n\n<p><br>\n<strong>You Will:</strong></p>\n\n<ul>\n\t<li>Help plan, build and maintain the next iteration of our data pipeline.</li>\n\t<li>Recommend and implement ways to improve data reliability, efficiency and quality.</li>\n\t<li>Collaborate with data architects, modelers and other team members on project goals.</li>\n</ul>\n\n<p><br>\n<strong>To join our team, you should:</strong></p>\n\n<ul>\n\t<li>Be motivated to excel individually and as part of a small team.</li>\n\t<li>Be looking to constantly grow and learn and enjoy bring others along with you.</li>\n</ul>"}