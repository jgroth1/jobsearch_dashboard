{"job title": "Data Engineer", "company": "Calico", "city state": "\u2013 South San Francisco, CA", "rating": "5.0", "job description": "<strong>Who we are:</strong>\n<br><br>\nCalico is a research and development company whose mission is to harness advanced technologies to increase our understanding of the biology that controls lifespan, and to devise interventions that enable people to lead longer and healthier lives. Executing on this mission will require an unprecedented level of interdisciplinary effort and a long-term focus for which funding is already in place.\n<br><br>\n<strong>Position Description:</strong>\n<br><br>\nGreat software engineering and data science are increasingly crucial to biology. We are in the midst of an explosion in the quantity and quality of biological and medical data that are transformative to our understanding of biology and disease. But the tools to store, process, and analyze these data are often primitive, and in some cases don't yet exist. Calico is seeking an exceptional data engineer to join our computing group and be a part of changing that story.\n<br><br>\nIn this role, you will work closely with computational and research scientists to define strategies and implement systems for modeling, collecting, storing, and accessing diverse scientific data and metadata. Collaborating with other scientists and engineers, you will design, build, and maintain databases and data warehouses that underpin our scientific endeavors and accelerate our ability to ask new, sophisticated questions spanning multiple organisms, data modalities, and timescales. You will not only build tools to support existing scientific workflows, but also help set the vision for future data generation and collection efforts.\n<br><br>\nIf you are passionate about data, passionate about biology, and passionate about their intersectionthis is the job for you.\n<br><br>\n<strong>What you'll do:</strong>\n\n<ul>\n<li>Work with computational and research scientists to understand common analysis use cases and data access needs.</li>\n<li>Design strategies for data storage and integration across different data sources (both internal and external) for multiple use cases.</li>\n<li>Implement, document, and maintain processing pipelines, databases, and data warehouse infrastructure.</li>\n<li>Work closely with full-stack engineers to develop APIs and GUIs for accessing and visualizing scientific data.</li>\n<li>Set data engineering vision and drive both independent and collaborative software development projects end-to-end.</li>\n<li>Contribute to a range of projects, from one-off solutions to long-term, complex systems.</li>\n<li>Build out core infrastructure, tooling, and software development processes.</li>\n</ul>\n<strong>Position requirements:</strong>\n\n<ul>\n<li>5+ years working with contemporary ETL tools and frameworks.</li>\n<li>3+ years building Python-based backend systems.</li>\n<li>Fluent knowledge of SQL.</li>\n<li>Experience implementing RESTful APIs, GraphQL, and other programmatic interfaces to complex multidimensional data.</li>\n<li>Experience deploying high-performance data backends in the cloud with Amazon Web Services, Heroku, Google Cloud Platform, or a similar service.</li>\n<li>Firm grasp on software testing and test-driven development.</li>\n<li>Demonstrated success in owning projects end-to-end, including working with non-technical stakeholders to define requirements and seek feedback.</li>\n</ul>\n<strong>Nice to have:</strong>\n\n<ul>\n<li>Worked with machine learning tools and infrastructure, e.g. TensorFlow and PyTorch.</li>\n<li>Built back-ends for high-dimensional graph or network data.</li>\n<li>Worked in biology or life sciences, and have familiarity with databases and data types used by computational biologists.</li>\n<li>Built software with technologies like ElasticSearch, GraphQL, and Google Cloud Platform.</li>\n</ul>\n<strong>Some projects you may contribute to:</strong>\n\n<ul>\n<li>Data warehousea system to extract, transform, and load public and private datasets into a single repository, then making these data available for analysis visually with either off-the-shelf or custom-built GUIs.</li>\n<li>Exploratory data visualization &amp; analysis toolsapps to help scientists explore and understand diverse, complex, and multidimensional data.</li>\n<li>Data platforma modern, React (front-end) and Python (back-end) application that our scientists use to manage and process experimental data.</li>\n</ul>\nAutomationsoftware to ingest and transform data from custom high-throughput instrumentation."}