{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize, wordpunct_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paras(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    for string in soup.stripped_strings:\n",
    "        yield string\n",
    "\n",
    "def sents(html):\n",
    "    for paragraph in paras(html):\n",
    "        for sentence in sent_tokenize(paragraph):\n",
    "            yield sentence\n",
    "\n",
    "def words(html):\n",
    "    for sentence in sents(html):\n",
    "        for token in wordpunct_tokenize(sentence):\n",
    "            yield token\n",
    "\n",
    "def tokenize(html):\n",
    "    for paragraph in paras(html):\n",
    "        yield [pos_tag(wordpunct_tokenize(sent)) for sent in sent_tokenize(paragraph)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "titles = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wnl = WordNetLemmatizer()\n",
    "n=0\n",
    "for i in range(10):\n",
    "    i += 1\n",
    "    path = 'corpus3/02-09-2018/Page_' + str(i) + '/'\n",
    "    files = os.listdir(path)\n",
    "    rxs = [re.search('^\\.',file) for file in files]\n",
    "    i =[n for n in range(len(rxs)) if rxs[n]]\n",
    "    m = 0\n",
    "    for k in i:\n",
    "        files.pop(k-m)\n",
    "        m += 1\n",
    "\n",
    "    for file in files: \n",
    "        with open(path + file, encoding='utf-8') as f:\n",
    "            job_post = f.read()\n",
    "\n",
    "        job_dict = eval(job_post)\n",
    "\n",
    "        job_dict.keys()\n",
    "\n",
    "        title = job_dict['job title']\n",
    "        company = job_dict['company']\n",
    "        titles.append([n, title, company])\n",
    "        html = job_dict['job description']\n",
    "        doc = []\n",
    "        xs = words(html)\n",
    "        for x in xs:\n",
    "            if x not in stop_words:\n",
    "                doc.append(wnl.lemmatize(x))\n",
    "\n",
    "        docs.append(doc)\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [d for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(docs)]\n",
    "model = Doc2Vec(documents, vector_size=50, min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "wnl = WordNetLemmatizer()\n",
    "n=0\n",
    "resses = []\n",
    "\n",
    "with open('/Users/grothjd/documents/DS_resume/Time_Series_Resume/Jonathan_Groth_PhD_Resume_test.txt', encoding='utf-8') as f:\n",
    "    resume = f.read()\n",
    "\n",
    "#print(resume)    \n",
    "    \n",
    "    \n",
    "xs = words(resume)\n",
    "for x in xs:\n",
    "    if x not in stop_words:\n",
    "        resses.append(wnl.lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/PythonData/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "inferred_vector = model.infer_vector(resses)\n",
    "sims = model.docvecs.most_similar([inferred_vector])\n",
    "\n",
    "select = []\n",
    "for sim in sims:\n",
    "    select.append(sim[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 'Data Scientist', 'Vesta Corporation'], [3, 'Data Scientist', 'Envision'], [10, 'Data Scientist', 'Beacon Hill Staffing'], [26, 'Data Scientist', 'Aptima'], [29, 'Data Scientist', 'Discovery Communications'], [35, 'Data Scientist', 'Jobspring Partners'], [51, 'Data Scientist', 'Hertz'], [222, 'BT-1582 Computer &amp; Information Research Scientist', 'Bastion Technologies'], [229, 'Data Scientistâž', '3m'], [248, 'Lead Data Eng-Demand Sensing, Enterprise Data', 'Nike']]\n"
     ]
    }
   ],
   "source": [
    "job = []\n",
    "for title in titles:\n",
    "    if int(title[0]) in select:\n",
    "        job.append(title)\n",
    "print(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
