{"job title": "Big Data Engineer", "company": "Credit Suisse", "city state": "\u2013 United States-NC-Raleigh", "rating": "3.6", "job description": "<p>We are seeking talented, experienced big data engineer to join a growing, high-visibility cross-Bank team that is developing and deploying solutions to some of Credit Suisse\u2019s most challenging analytic and big data problems. As a member of this team, you will work with clients and data spanning Credit Suisse\u2019s global organization to solve emerging mission-critical challenges via the utilization of emerging technologies such as: </p>\n<ul>\n<li>\n<div>Distributed file systems and storage technologies (HDFS, HBase, Accumulo, Hive).</div></li>\n<li>\n<div>Large-scale distributed data analytic platforms and compute environments (Spark, Map/Reduce).</div></li>\n<li>\n<div>Tools for semantic reasoning and ontological data normalization (RDF, SPARQL, Tamr).</div></li></ul>\n<p> </p>\n<p>The role offers:</p>\n<ul>\n<li>\n<div>A hands-on engineering position responsible for supporting client engagements for Big Data engineering and planning.</div></li>\n<li>\n<div>A solid platform for you to drive the engineering/design decisions needed to achieve cost-effective and high performance result.</div></li>\n<li>\n<div>Thinking out of the box on improvements to current processes &amp; enhancing existing platform.</div></li>\n<li>\n<div>You will be part of a global team of Big Data engineers who are engineering the platform and innovating in core areas of big data, real time analytics and large-scale data processing.</div></li></ul>\n<p> </p>\n<p>Credit Suisse maintains a Working Flexibility Policy, subject to the terms as set forth in the Credit Suisse United States Employment Handbook.<br></p>\n<ul>\n<li>You have a formal background and proven experience in engineering, mathematics and computer science, particularly within the financial services sector.</li>\n<li>You have hands on Programming/Scripting Experience (Python, Java, Scala, Bash).</li>\n<li>DevOps Tools (Chef, Docker, Puppet, Bamboo, Jenkins).</li>\n<li>Linux/Windows (Command line). An understanding of Unix/Linux including system administration and shell scripting.</li>\n<li>You gave proficiency with Hadoop v2, MapReduce, HDFS, Spark.</li>\n<li>Management of Hadoop cluster, with all included services.</li>\n<li>You have good knowledge of Big Data querying tools, such as Pig, Hive, Impala and Spark.</li>\n<li>Data Concepts (ETL, near-/real-time streaming, data structures, metadata and workflow management).</li>\n<li>You have the ability to function within a multidisciplinary, global team. Be a self-starter with a strong curiosity for extracting knowledge from data and the ability to elicit technical requirements from a non-technical audience.</li>\n<li>Collaboration with team members, business stakeholders and data SMEs to elicit, translate, and prescribe requirements. Cultivate sustained innovation to deliver exceptional products to customers.</li>\n<li>Do you have experience with integration of data from multiple data sources?</li>\n<li>Do you have strong communication skills and the ability to present deep technical findings to a business audience?</li></ul>\n<p> </p>\n<p>For more information visit Technology Careers<br></p>"}