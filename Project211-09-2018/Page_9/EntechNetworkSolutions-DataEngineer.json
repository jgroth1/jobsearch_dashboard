{"job title": "Data Engineer", "company": "Entech Network Solutions", "city state": "\u2013 Reston, Virginia", "rating": "4.8", "job description": "<strong>The role</strong><br><br><strong>We appreciate the value of data within ours and client\u2019s business. We generate more data than ever before but we recognise that clean and meaningful data streamed at a rate which can give us a competitive edge is the key. Our Data Engineers are key to activating our innovative solutions and the role requires an individual who can work with other analysts to create solutions which integrate with our advertising technology stack.</strong><br><br><strong>Responsibilities</strong> \n\n\n<ul>\n\n\t\n<li><strong>Responsible for all extract, transform and load (ETL) processes and the creation of applications that can connect to remote APIs. Preferably including DoubleClick, Google Analytics, AdWords and Google Tag-Manager and stream data into environments such as BigQuery.</strong></li>\n\n\n\n<li><strong>Responsible for the management of multiple processes and applications, performance reporting and error checking.</strong></li>\n\n\n\n<li><strong>Responsible for the management of all data created within client applications, the structure of data held and the views of data created.</strong></li>\n\n\n\n<li><strong>Responsible for recommending the correct technologies to be used and in the most cost effective manner.</strong></li>\n\n\n\n<li><strong>Responsible for the design and creation of data led strategies which provide clients with opportunities to leverage their data for greater insight or performance.</strong></li>\n\n\n\n<li><strong>Provide thought leadership with regards to best practice and use of the google cloud platform.</strong></li>\n\n\n\n</ul>\n\n<strong>Knowledge &amp; Experience Requirements</strong>\n\n\n<ul>\n\n\t\n<li><strong>Must have experience of building robust pipelines and data environments to support database or machine learning based applications.</strong></li>\n\n\n\n</ul>\n\n<strong>Skills requirements</strong> \n\n\n<ul>\n\n\t\n<li><strong>Data Engineering/ BI Development/ Data Warehousing experience. Knowledge of serverless infrastructure beneficial</strong></li>\n\n\n\n<li><strong>Ability to scope a project based on a technical brief and work with the DevOps and QA teams to provide a detailed project plan including:</strong>\n\n<ul>\n\n \n<li><strong>Data Flow Diagrams for process flow</strong></li>\n\n\n\n<li><strong>Database Schemas &amp; Normalization</strong></li>\n\n\n\n<li><strong>Recommended software / plugins / architecture</strong></li>\n\n\n\n<li><strong>Scalable environment architecture suggestions</strong></li>\n\n\n\n<li><strong>Hosting, storage, load balancing and caching suggestions</strong></li>\n\n\n\n<li><strong>Performance considerations</strong></li>\n\n\n\n<li><strong>Security considerations</strong></li>\n\n\n\n<li><strong>Assumptions &amp; Exclusions</strong></li>\n\n\n\n<li><strong>A complete and accurate estimate for the project</strong></li>\n\n\n\n</ul>\n\n\t</li>\n<li><strong>Ability to assess new business and respond with a full list of targeted questions to ensure accurate estimates are created</strong></li>\n\n\n</ul>\n\n\n\n<ul>\n\n\t\n<li><strong>Ability to research solutions to technical problems</strong></li>\n\n\n\n<li><strong>Experience scheduling/automating scripts</strong></li>\n\n\n\n<li><strong>Experience with streaming data beneficial</strong></li>\n\n\n\n<li><strong>Experience on Linux command line and Bash scripting</strong></li>\n\n\n\n<li><strong>Experience with Git/GitHub</strong></li>\n\n\n\n<li><strong>Experience with Amazon/Google Cloud services. Experience with Dataflow, Google PubSub or other queuing software beneficial</strong></li>\n\n\n\n<li><strong>Good experience of parsing data formats such as XML/JSON and using 3rd party API\u2019s</strong>\n\n<ul>\n\n \n<li><strong>Experience with Curl / similar beneficial</strong></li>\n\n\n\n</ul>\n\n\t</li>\n<li><strong>Solid Python programming skills. Java / other languages beneficial.</strong></li>\n\n\n\n<li><strong>Strong SQL experience, any flavour.</strong></li>\n\n\n\n<li><strong>Experience in using Key/Value or Document Stores such as DocumentDB, BigTable, NoSQL, MongoDB, Hadoop</strong></li>\n\n\n\n<li><strong>Basic experience with Tensorflow. CloudML/Spark/SparkML Beneficial</strong></li>\n\n\n</ul>\n\n\n\n<ul>\n\n\t\n<li><strong>An understanding of how data can benefit the wider business, and how to translate technical requirements to non-technical stakeholders.</strong></li>\n\n\n\n</ul>\n\n<strong>Key Attributes</strong>\n\n\n<ul>\n\n\t\n<li><strong>A self-motivated individual with high levels of energy.</strong></li>\n\n\n\n<li><strong>An outgoing, curious thinker, with consistent levels of evident enthusiasm.</strong></li>\n\n\n\n<li><strong>Flexible, versatile and copes well under pressure.</strong></li>\n\n\n\n<li><strong>Open, honest and direct, is comfortable in giving and receiving constructive feedback.</strong></li>\n\n\n\n<li><strong>Has high personal standards and promotes them in others.</strong></li>\n\n\n\n<li><strong>A valued team member, who upholds Jellyfish\u2019s values and professional integrity at all times.</strong></li>\n\n\n\n<li><strong>Strong verbal and communication skills \u2013 able to communicate with clients effectively</strong></li>\n\n\n\n<li><strong>Pro-active &amp; self-motivated with a sense of ownership</strong></li>\n\n\n\n<li><strong>Adopts a \u201cleave code better than you found it (and fully commented)\u201d attitude</strong></li>\n\n\n\n</ul>\n\n<strong>T</strong>"}