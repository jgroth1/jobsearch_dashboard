{"job title": "Data Engineer", "company": "Atlassian", "city state": "\u2013 San Francisco, United States", "rating": "3.9", "job description": "Atlassian is looking for a Data Engineer to join our Marketing Data Engineering team and build world-class data solutions and applications that powers crucial business decisions throughout the organization. We are looking for an open minded, structured thinker who is passionate about building systems at scale. You'll be the genius who understands data at Atlassian, knows where to find it, and manages the process to make that data useful for Analytics. You love thinking about the ways the business can consume this data and then figuring out how to build it.\n<br><br>\nOn a typical day you will be building the data models and ETL processes to provide this data for business use. You've got industry experience working with large datasets. You are interested in reporting platforms and data visualization. As the data domain expert, you will be partnering with our technology teams, analytical teams, and data scientists across various initiatives.\n<br><br>\nYou'll own a problem end-to-end, so those skills will come in handy not just to collect, extract, and clean the data, but also to understand the systems that generated it, and automate your analyses and reporting. On an on-going basis, you'll be responsible for improving the data by adding new sources, coding business rules, and producing new metrics that support the business. Requirements will be vague. Iterations will be rapid. You will need to be nimble and take smart risks.\n<br><br>\n<strong>More about you</strong>\n<br><br>\nAs a data engineer, you may have experience spanning traditional DW and ETL architectures. But for this role it is important to have industry experience working with big data ecosystems like Spark/Hadoop and Redshift. You've probably been in the industry as an engineer for 2+ years and have developed a passion for the data that drives businesses.\n<br><br>\nOn your first day, we'll expect you to have:\n<ul>\n<li>Deep understanding of big data challenges and eco-system</li><li>Experience with solution building and architecting with public cloud offerings such as Amazon Web Services, Redshift, S3, EMR/Spark, Presto/Athena</li><li>Experience with Spark and Hive</li><li>Expertise in SQL, SQL tuning, schema design, Python and ETL processes</li><li>Expertise in data pipeline with such workflow tools as Airflow, Oozie or Luigi</li><li>Solid understanding experience in building RESTful APIs and microservices, e.g. with Flask</li><li>Experience in test automation and ensuring data quality across multiple datasets used for analytical purposes</li><li>Experience with Lambda Architecture or other Big Data architectural best practices</li><li>A graduate degree in Computer Science or similar discipline</li><li>Commit code to open source projects</li><li>Experience with test automation and continuous delivery</li>\n</ul>\nIt's great, but not required, if you have:\n<ul>\n<li>Experience with Tableau</li><li>Experience with Machine Learning</li><li>Have worked with Data Scientists</li>\n</ul>"}